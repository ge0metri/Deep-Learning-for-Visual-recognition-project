{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model that uses the fenchel-young loss and trains on 200 images \n",
    "# the images are permuted in 9 tiles and in every iteration a new permutation is generated.\n",
    "# we use a deep encoder with fixed parameters, adam optimizer, preprocessing before tiling  \n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple___Apple_scab Apple___Black_rot Apple___Cedar_apple_rust Apple___healthy 1000\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "number_of_images = 1000\n",
    "\n",
    "root = os.getcwd() # Don't change this\n",
    "data_dirname = '/data_test/plantvillage/' # Change as you like \n",
    "p = Path(root + data_dirname)\n",
    "p.mkdir(exist_ok=True) \n",
    "if platform.system()=='Darwin':\n",
    "  root = os.getcwd() # Don't change this\n",
    "  data_dirname = '/data_test/plantvillage/' # Change as you like \n",
    "  p = Path(root + data_dirname)\n",
    "  p.mkdir(exist_ok=True) \n",
    "else:\n",
    "  #p = Path(\"C:/Users/mwels/Documents/Uni/11. Semester/Deep learning in visual recognition/Plant_leave_diseases_dataset_without_augmentation\")\n",
    "  #p.mkdir(exist_ok=True)\n",
    "  pass\n",
    "\n",
    "\n",
    "\n",
    "classes = [\n",
    "  'Apple___Apple_scab',\n",
    "  'Apple___healthy',\n",
    "  'Apple___Black_rot',\n",
    "  'Apple___Cedar_apple_rust',\n",
    "  \"all\"\n",
    "  ]\n",
    "\n",
    "if \"all\" in classes:\n",
    "  classes = os.listdir(p)\n",
    "\n",
    "for c in classes:\n",
    "  print(c,end=\" \")\n",
    "  filelist = [x for x in (p/c).iterdir() if x.is_file()]\n",
    "  for f in filelist:\n",
    "    img = cv2.imread(str(f))\n",
    "    if img is None:\n",
    "      print(f'Failed to open {f}. Deleting file')\n",
    "      os.remove(str(f))\n",
    "\n",
    "\n",
    "filelist = filelist[:number_of_images]\n",
    "print(len(filelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tilex = 3\n",
    "number_of_tiles = tilex**2\n",
    "number_of_permutations = 30 \n",
    "target_siz = (224,224,3)\n",
    "tile_size = target_siz[0]//tilex\n",
    "sfmax = True\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "conv_base = MobileNet(weights='imagenet',\n",
    "                      include_top=False,\n",
    "                      input_shape=target_siz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Concatenate, Input, Flatten, Lambda, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "import os\n",
    "\n",
    "\n",
    "#tiles = Input((number_of_tiles,tile_size,tile_size,3))\n",
    "tiles = Input(target_siz)\n",
    "\n",
    "inputs = {}\n",
    "layers = {}\n",
    "embedds = {}\n",
    "\n",
    "shared_conv = conv_base \n",
    "\n",
    "# for i in range(number_of_tiles):\n",
    "#     #inputs[f'tiles{i}'] = Input((tile_size,tile_size,3))\n",
    "#     #layers[f'tile{i}'] = Lambda(lambda x: x[:,i,:,:,:])(tiles)\n",
    "\n",
    "#     #layers[f'deep_layers{i}'] = shared_conv(inputs[f'tiles{i}'])\n",
    "#     layers[f'deep_layers{i}'] = shared_conv(layers[f'tile{i}'])\n",
    "#     embedds[f'embedd{i}'] = Flatten()(layers[f'deep_layers{i}'])\n",
    "#concatonation = Concatenate(axis=1)(list(embedds.values()))\n",
    "\n",
    "concatonation = shared_conv(tiles)\n",
    "concatonation = GlobalAveragePooling2D()(concatonation)\n",
    "\n",
    "out = Dense(number_of_tiles*10, activation=\"relu\", kernel_initializer='he_normal')(concatonation)\n",
    "if sfmax:\n",
    "    out = Dense(number_of_permutations, activation=\"softmax\", kernel_initializer='he_normal')(out)\n",
    "else:\n",
    "    out = Dense(number_of_tiles, kernel_initializer='he_normal')(out)\n",
    "out = Flatten()(out)\n",
    "\n",
    "model = Model(inputs=tiles, outputs=out)\n",
    "#model = Model(inputs=list(inputs.values()), outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, \n",
    "    show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 7, 7, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1024)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 90)                92250     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                2730      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 30)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,323,844\n",
      "Trainable params: 3,301,956\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of layers is 6\n",
      "Number of pretrained base layers is 86\n"
     ]
    }
   ],
   "source": [
    "total_num_layers = len(model.layers)\n",
    "num_base_layers = len(conv_base.layers)\n",
    "print(f\"Total number of layers is {total_num_layers}\")\n",
    "print(f\"Number of pretrained base layers is {num_base_layers}\")\n",
    "\n",
    "for layer in model.layers[:3]:\n",
    "    layer.trainable=False\n",
    "for layer in model.layers[3:]:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.PermOneHotDataGen import *\n",
    "from src.model_tools import *\n",
    "from src.permutation_tools import *\n",
    "\n",
    "x_train, x_test = train_test_split(filelist)\n",
    "\n",
    "train_generator = PermOneHotDataGen(x_train,\n",
    "                                    batch_size=8,\n",
    "                                    tilenumberx=tilex, \n",
    "                                    shuffle_permutations=True)\n",
    "\n",
    "validation_generator = PermOneHotDataGen(x_test,\n",
    "                                        batch_size=8,\n",
    "                                        tilenumberx=tilex,\n",
    "                                        shuffle_permutations=True)\n",
    "\n",
    "train_generator_sm = PermOneHotDataGen(x_train,\n",
    "                                    batch_size=1,\n",
    "                                    tilenumberx=tilex, max_perms=number_of_permutations)\n",
    "\n",
    "validation_generator_sm = PermOneHotDataGen(x_test,\n",
    "                                        batch_size=1,\n",
    "                                        tilenumberx=tilex, max_perms=number_of_permutations)\n",
    "\n",
    "train_stitch_sm = PermOneHotDataGen(x_train,\n",
    "                                    batch_size=16,\n",
    "                                    tilenumberx=tilex,\n",
    "                                    shuffle_permutations=True,\n",
    "                                    max_perms=number_of_permutations,\n",
    "                                    target_size=target_siz, \n",
    "                                    stitched=True,\n",
    "                                    one_hot_encoding=True,\n",
    "                                    Permutations_dictionary=None)\n",
    "\n",
    "validation_stitch_sm = PermOneHotDataGen(x_test,\n",
    "                                        batch_size=len(x_test),\n",
    "                                        tilenumberx=tilex, \n",
    "                                        max_perms=number_of_permutations, \n",
    "                                        target_size=target_siz, \n",
    "                                        stitched=True,\n",
    "                                        one_hot_encoding=True,\n",
    "                                        Permutations_dictionary=train_stitch_sm.get_perm_dict())\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    name='Adam',\n",
    ")\n",
    "\n",
    "if sfmax:\n",
    "    model.compile(optimizer=optimizer,\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=['accuracy'])\n",
    "else:\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=RankingLoss(),\n",
    "        metrics=[ProjectedRanksAccuracy(), PartialRanksAccuracy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 9)\n",
      "(1, 9)\n",
      "(8, 9)\n",
      "(1, 9)\n",
      "(8, 9, 85, 85, 3)\n",
      "(1, 9, 85, 85, 3)\n",
      "(8, 9, 85, 85, 3)\n",
      "(1, 9, 85, 85, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_generator.next()[1].shape)\n",
    "print(train_generator_sm.next()[1].shape)\n",
    "print(validation_generator.next()[1].shape)\n",
    "print(validation_generator_sm.next()[1].shape)\n",
    "print(train_generator.next()[0].shape)\n",
    "print(train_generator_sm.next()[0].shape)\n",
    "print(validation_generator.next()[0].shape)\n",
    "print(validation_generator_sm.next()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProjectedRanksAccuracy().update_state(validation_generator.next()[1], validation_generator.next()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class PlotLearning(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback to plot the learning curves of the model during training.\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.metrics = {}\n",
    "        for metric in logs:\n",
    "            self.metrics[metric] = []\n",
    "            \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Storing metrics\n",
    "        for metric in logs:\n",
    "            if metric in self.metrics:\n",
    "                self.metrics[metric].append(logs.get(metric))\n",
    "            else:\n",
    "                self.metrics[metric] = [logs.get(metric)]\n",
    "        \n",
    "        # Plotting\n",
    "        metrics = [x for x in logs if 'val' not in x]\n",
    "        \n",
    "        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            axs[i].plot(range(1, epoch + 2), \n",
    "                        self.metrics[metric], \n",
    "                        label=metric)\n",
    "            if logs['val_' + metric]:\n",
    "                axs[i].plot(range(1, epoch + 2), \n",
    "                            self.metrics['val_' + metric], \n",
    "                            label='val_' + metric)\n",
    "                \n",
    "            axs[i].legend()\n",
    "            axs[i].grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "47/47 [==============================] - 33s 648ms/step - loss: 3.1888 - accuracy: 0.1320 - val_loss: 2.8019 - val_accuracy: 0.2080\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 40s 856ms/step - loss: 2.1871 - accuracy: 0.4507 - val_loss: 2.0581 - val_accuracy: 0.4320\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 38s 813ms/step - loss: 1.4147 - accuracy: 0.7067 - val_loss: 1.5937 - val_accuracy: 0.5640\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 31s 670ms/step - loss: 0.8771 - accuracy: 0.8600 - val_loss: 1.2552 - val_accuracy: 0.6440\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 30s 647ms/step - loss: 0.5750 - accuracy: 0.9333 - val_loss: 1.0416 - val_accuracy: 0.7280\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 34s 725ms/step - loss: 0.3875 - accuracy: 0.9747 - val_loss: 0.9307 - val_accuracy: 0.7560\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 36s 760ms/step - loss: 0.2713 - accuracy: 0.9933 - val_loss: 0.7574 - val_accuracy: 0.7880\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 37s 784ms/step - loss: 0.1933 - accuracy: 0.9987 - val_loss: 0.7000 - val_accuracy: 0.8240\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 28s 605ms/step - loss: 0.1444 - accuracy: 1.0000 - val_loss: 0.7172 - val_accuracy: 0.8040\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 28s 604ms/step - loss: 0.1110 - accuracy: 1.0000 - val_loss: 0.6377 - val_accuracy: 0.8160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1960a8ee860>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_epochs = 10\n",
    "\n",
    "model.fit(train_stitch_sm,\n",
    "          epochs = nb_epochs,\n",
    "          validation_data=validation_stitch_sm,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"models/2022_10_27__01\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('visual')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64359c18de76bb4d3549d4f42519674c79e06566b1fdefa8f4209e868c4da35b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
